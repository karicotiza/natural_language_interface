{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import main\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "             word  target\n0      annotation       1\n1               в       1\n2         котором       1\n3      творческие       1\n4        принципы       1\n...           ...     ...\n24580      public       0\n24581    orgastic       0\n24582     recedes       0\n24583       boats       0\n24584       borne       0\n\n[24585 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>annotation</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>в</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>котором</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>творческие</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>принципы</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>24580</th>\n      <td>public</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>24581</th>\n      <td>orgastic</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>24582</th>\n      <td>recedes</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>24583</th>\n      <td>boats</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>24584</th>\n      <td>borne</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>24585 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# en = main.SearchImageOfLanguage(\n",
    "#     \"data/Fitzgerald - Gatsby.pdf\",\n",
    "#     length_threshold=20,\n",
    "#     amount_threshold=1,\n",
    "# )\n",
    "#\n",
    "# ru = main.SearchImageOfLanguage(\n",
    "#     \"data/Достоевский - Идиот.pdf\",\n",
    "#     length_threshold=20,\n",
    "#     amount_threshold=1,\n",
    "# )\n",
    "#\n",
    "# en.save_as(\"en_data\")\n",
    "# ru.save_as(\"ru_data\")\n",
    "\n",
    "ru_data = pd.read_feather(\"data/ru_data.feather\")\n",
    "en_data = pd.read_feather(\"data/en_data.feather\")\n",
    "\n",
    "ru_data.drop(\n",
    "    [\n",
    "        \"amount\",\n",
    "        \"frequency\",\n",
    "    ],\n",
    "    axis=1,\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "en_data.drop(\n",
    "    [\n",
    "        \"amount\",\n",
    "        \"frequency\",\n",
    "    ],\n",
    "    axis=1,\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "ru_data[\"target\"] = 1\n",
    "en_data[\"target\"] = 0\n",
    "\n",
    "data = pd.concat([ru_data, en_data], ignore_index=True)\n",
    "data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow - 2.9.0\n",
      "Keras - 2.9.0\n",
      "GPU - On\n",
      "CUDA - 64_112\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"TensorFlow - {tf.__version__}\\n\"\n",
    "    f\"Keras - {tf.keras.__version__}\"\n",
    ")\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print(\"GPU - On\")\n",
    "    # TensorFlow / CUDA / CUDnn compatibility table - https://www.tensorflow.org/install/source#gpu\n",
    "    print(f\"CUDA - {tf.sysconfig.get_build_info()['cuda_version']}\")\n",
    "else:\n",
    "    print(\"GPU - Off\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train inputs shape - (19668,)\n",
      "Train targets shape - (19668,)\n",
      "\n",
      "Test inputs shape - (4917,)\n",
      "Test targets shape - (4917,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_inputs, test_inputs, train_targets, test_targets = train_test_split(data[\"word\"], data[\"target\"], test_size=0.2, random_state=42, stratify=data[\"target\"])\n",
    "\n",
    "shape = train_inputs.shape[1:]\n",
    "\n",
    "print(\n",
    "    f\"Train inputs shape - {train_inputs.shape}\\n\"\n",
    "    f\"Train targets shape - {train_targets.shape}\\n\\n\"\n",
    "    f\"Test inputs shape - {test_inputs.shape}\\n\"\n",
    "    f\"Test targets shape - {test_targets.shape}\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "num_words = 25000\n",
    "max_len = 20\n",
    "nb_classes = 2\n",
    "\n",
    "train_targets = to_categorical(train_targets, nb_classes)\n",
    "test_targets = to_categorical(test_targets, nb_classes)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(data[\"word\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(train_inputs)\n",
    "train_inputs = pad_sequences(sequences, maxlen=max_len)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers, models, layers, backend, callbacks\n",
    "\n",
    "backend.clear_session()\n",
    "\n",
    "model = models.Sequential(name=\"language_detection\")\n",
    "\n",
    "model.add(layers.Embedding(num_words, 32, input_length=max_len))\n",
    "model.add(layers.GRU(16))\n",
    "model.add(layers.Dense(units=2, activation=\"softmax\"))\n",
    "\n",
    "# model.build(input_shape=(1, ))\n",
    "\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "checkpoint = callbacks.ModelCheckpoint(\n",
    "    'best_model.hdf5' ,\n",
    "    monitor = 'accuracy',\n",
    "    verbose = True,\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "early_stop = callbacks.EarlyStopping(\n",
    "    monitor='accuracy',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "callbacks_list = [checkpoint, early_stop]\n",
    "\n",
    "# model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "615/615 [==============================] - ETA: 0s - loss: 0.5277 - accuracy: 0.7841\n",
      "Epoch 1: accuracy improved from -inf to 0.78412, saving model to best_model.hdf5\n",
      "615/615 [==============================] - 5s 5ms/step - loss: 0.5277 - accuracy: 0.7841\n",
      "Epoch 2/5\n",
      "605/615 [============================>.] - ETA: 0s - loss: 0.1788 - accuracy: 0.9329\n",
      "Epoch 2: accuracy improved from 0.78412 to 0.93375, saving model to best_model.hdf5\n",
      "615/615 [==============================] - 3s 5ms/step - loss: 0.1767 - accuracy: 0.9338\n",
      "Epoch 3/5\n",
      "612/615 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9996\n",
      "Epoch 3: accuracy improved from 0.93375 to 0.99959, saving model to best_model.hdf5\n",
      "615/615 [==============================] - 3s 5ms/step - loss: 0.0017 - accuracy: 0.9996\n",
      "Epoch 4/5\n",
      "605/615 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 4: accuracy improved from 0.99959 to 0.99964, saving model to best_model.hdf5\n",
      "615/615 [==============================] - 3s 5ms/step - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 5/5\n",
      "610/615 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9995\n",
      "Epoch 5: accuracy did not improve from 0.99964\n",
      "615/615 [==============================] - 3s 5ms/step - loss: 0.0016 - accuracy: 0.9995\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_inputs,\n",
    "    train_targets,\n",
    "    verbose=True,\n",
    "    epochs=5,\n",
    "    callbacks=[\n",
    "        callbacks_list,\n",
    "    ],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(test_inputs)\n",
    "test_inputs = pad_sequences(test_sequences, maxlen=max_len)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "my_model = models.load_model(\"best_model.hdf5\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train inputs shape - (19668, 20)\n",
      "Train targets shape - (19668, 2)\n",
      "\n",
      "Test inputs shape - (4917, 20)\n",
      "Test targets shape - (4917, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Train inputs shape - {train_inputs.shape}\\n\"\n",
    "    f\"Train targets shape - {train_targets.shape}\\n\\n\"\n",
    "    f\"Test inputs shape - {test_inputs.shape}\\n\"\n",
    "    f\"Test targets shape - {test_targets.shape}\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154/154 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.7092\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.6933085322380066, 0.709172248840332]"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model.evaluate(test_inputs, test_targets, verbose=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0 10445]\n",
      "(20,)\n"
     ]
    }
   ],
   "source": [
    "print(test_inputs[0])\n",
    "print(test_inputs[0].shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 11ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[9.9979240e-01, 2.0759509e-04]], dtype=float32)"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"fields\"\n",
    "\n",
    "my_array = [0 for _ in range(19)]\n",
    "my_array.append(tokenizer.word_index[word])\n",
    "\n",
    "a = np.array(my_array).reshape(1, max_len)\n",
    "\n",
    "my_model.predict(a)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "# import pickle\n",
    "#\n",
    "# # saving\n",
    "# with open('tokenizer.pickle', 'wb') as handle:\n",
    "#     pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# # loading\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    aboba = pickle.load(handle)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "24570"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aboba.word_index[\"fields\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
